{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "category_dict = {\"안전/환경\": 1,\"미래\": 2,\"일자리\": 3,\"보건복지\": 4,\"정치개혁\": 5,\"경제민주화\": 6,\"인권/성평등\": 7,\"외교/통일/국방\": 8,\"육아/교육\": 9,\"문화/예술/체육/언론\": 10,\"반려동물\": 11,\"교통/건축/국토\": 12,\"행정\": 13,\"농산어촌\": 14,\"저출산/고령화대책\": 15,\"성장동력\": 16,\"기타\": 17}\n",
    "with open('/home/minwookje/다운로드/original.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277030\n"
     ]
    }
   ],
   "source": [
    "total = list()\n",
    "for i in data:\n",
    "    k = {\"title\":data[i][\"title\"],\"category\":data[i][\"category\"],\"content\": data[i][\"content\"]}\n",
    "    total.append(k)\n",
    "# 277030 문장    \n",
    "print(len(total)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dict()\n",
    "for k in category_dict.keys():\n",
    "    sub = list()\n",
    "    for i in total:\n",
    "        if i[\"category\"]== k:\n",
    "#             print(type(i[k]))\n",
    "#             print(i[k])\n",
    "#             break\n",
    "            sub.extend([i[\"content\"]])\n",
    "#             sub.extend([i[\"content\"]+\"  \"])\n",
    "#     break\n",
    "    corpus[k] = sub\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "``` python\n",
    "# text2sparse_matrix(make term frequency matrix==TDM)\n",
    "# 문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW 인코딩한 벡터를 만든다.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    #특정 단어를 포함한 문서의 비율. if min_df = 0.02(100개의 문서에서 2번등장한 단어까지 matrix에 포함)\n",
    "    min_df=0,\n",
    "    #max_df = 0.5, 51개 이상의 문서에서 등장한 단어들은 제외된다.\n",
    "    max_df=1,\n",
    "    #TDM에서 이용할 n-gram의 범위\n",
    "    #default: uni, if (1,3)== uni,bi,trigram모두 이용\n",
    "    ngram_range=(1,1),\n",
    "    lowercase=True,\n",
    "    #한국어에서는 이 tokenizer가 가장 중요하다. \n",
    "    tokenizer=lambda x:x.split())\n",
    "\n",
    "corpus.iter_sent=False\n",
    "# fit or fit_transform() input data type = list of str(DoublespaceLineCorpus output = str yield)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape) # (100, 7624)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer\n",
    "``` python\n",
    "#TF-idf\n",
    "# CountVectorizer를 이용하여 만들어진 TDM을 tfidf형식으로 변환.\n",
    "# CountVectorizer와 비슷하지만 TF-IDF 방식으로 단어의 가중치를 조정한 BOW 벡터를 만든다\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=0,\n",
    "    max_df=1,\n",
    "    ngram_range=(1,1),\n",
    "    lowercase=True,\n",
    "    tokenizer=lambda x:x.split())\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .fit() or .fit_transform()\n",
    "```python\n",
    "#  각 문서에 어떤 말들이 등장하였는지 정확히 살펴보기 위해 CountVectorizer 를 이용하여 \n",
    "# term frequency matrix 를 먼저 만들어 저장을 해두고, \n",
    "# 필요시에 따라 tfidf 로 변환하여 이용.\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# sparse matrix 의 각 column 이 어떤 단어에 해당하는지에 대한 index 가 저장\n",
    "# .fit()함수를 통하여 위의 결과 진행.\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "X = transformer.fit_transform(X)\n",
    "\n",
    "# .vocabulary_ {str:int}형식으로 각 단어 당 inx 저장 dict\n",
    "vectorizer.vocabulary_\n",
    "```\n",
    "ex)\n",
    "`{'19': 129,\n",
    " '1990': 149,\n",
    " '52': 478,\n",
    " '22': 265,\n",
    " '오패산터널': 5944,\n",
    " '총격전': 8150,\n",
    " ...\n",
    "}`\n",
    "\n",
    "> 각 idx가 어떤 단어인지를 저장하는 list or str생성 가능. \n",
    "띄어쓰기 기준으로 tokenizer 하여 비슷한 어절 모두 다른 단어로 학습됨.\n",
    "\n",
    "```python\n",
    "idx2vocab = [vocab for vocab, idx in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])]\n",
    "print(idx2vocab[5537:5542])\n",
    "# ['어려운', '어려움을', '어려움이', '어려워', '어려웠다']\n",
    "```\n",
    "\n",
    "> 이후 Vectorizer 저장  with pickling\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "with open('./vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "```\n",
    "\n",
    "> Vectorizer 에 의하여 만들어지는 term frequency matrix 는 scipy.sparse.csr.csr_matrix 형식, tdm 저장 법\n",
    "\n",
    "```python\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "#type\n",
    "print(type(X))\n",
    "# scipy.sparse.csr.csr_matrix\n",
    "\n",
    "\n",
    "# save\n",
    "from scipy.io import mmwrite\n",
    "\n",
    "mtx_path = './x.mtx'\n",
    "mmwrite(mtx_path, X)\n",
    "```\n",
    "\n",
    "\n",
    "> 저장된 matrix를 읽어올때, mmread()를 이용하면 coo matrix형식\n",
    "\n",
    "> csr형식으로 변환하고 싶다면 tocsr()사용\n",
    "\n",
    "```python\n",
    "#load\n",
    "\n",
    "from scipy.io import mmread\n",
    "\n",
    "X = mmread(mtx_path)\n",
    "print(type(X))\n",
    "# scipy.sparse.coo.coo_matrix\n",
    "\n",
    "X = X.tocsr()\n",
    "print(type(X))\n",
    "# scipy.sparse.csc.csc_matrix\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn vectorizer + KoNLPy\n",
    "\n",
    "> konlpy의 tokenizer를 활용한 scikit-learn vectorizer를 사용하기 위해서 MyTokenizer를 생성해 준다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt as Twitter\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "    def __call__(self, sent):\n",
    "#         pos = self.tagger.pos(sent)\n",
    "        pos = self.tagger.nouns(sent)\n",
    "#         pos = ['{}/{}'.format(word,tag) for word, tag in pos]\n",
    "        pos = ['{}'.format(word) for word in pos]\n",
    "#         여기에 noun추가\n",
    "        return pos\n",
    "\n",
    "# my_tokenizer = MyTokenizer(Twitter())\n",
    "\n",
    "# sent = '이건테스트문장입니다.'\n",
    "# print(my_tokenizer(sent))\n",
    "# ['이건/Noun', '테스트/Noun', '문장/Noun', '입니/Adjective', '다/Eomi', './Punctuation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> idx2vocab list를 생성해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "안전/환경 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_1.pkl is saved\n",
      "미래 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_2.pkl is saved\n",
      "일자리 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_3.pkl is saved\n",
      "보건복지 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_4.pkl is saved\n",
      "정치개혁 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_5.pkl is saved\n",
      "경제민주화 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_6.pkl is saved\n",
      "인권/성평등 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_7.pkl is saved\n",
      "외교/통일/국방 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_8.pkl is saved\n",
      "육아/교육 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_9.pkl is saved\n",
      "문화/예술/체육/언론 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_10.pkl is saved\n",
      "반려동물 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_11.pkl is saved\n",
      "교통/건축/국토 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_12.pkl is saved\n",
      "행정 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_13.pkl is saved\n",
      "농산어촌 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_14.pkl is saved\n",
      "저출산/고령화대책 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_15.pkl is saved\n",
      "성장동력 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_16.pkl is saved\n",
      "기타 is vectorizing!\n",
      "vectorizing is finished\n",
      "./padded_17.pkl is saved\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt as Twitter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from scipy.io import mmwrite\n",
    "\n",
    "# my_tokenizer = MyTokenizer(Twitter())\n",
    "# if tf matrix\n",
    "# vectorizer = CountVectorizer(tokenizer = my_tokenizer)\n",
    "#  if tf-idf matrix\n",
    "# vectorizer = TfidfVectorizer(tokenizer = my_tokenizer)\n",
    "# x = vectorizer.fit_transform(corpus['안전/환경'])\n",
    "\n",
    "# my_tokenizer = MyTokenizer(Twitter())\n",
    "# corpus2 = dict()\n",
    "# # # print(my_tokenizer(sent))\n",
    "# for k in corpus.keys():\n",
    "#     print(str(k)+\" is tokenizing\")\n",
    "#     sub = list()\n",
    "#     for i in corpus[k]:\n",
    "#         tmp = my_tokenizer(i)\n",
    "#     #     띄어쓰기로 분리된 \n",
    "#         sub.append(' '.join(tmp))\n",
    "#     corpus2[k] = sub\n",
    "#     print(str(k)+\" is tokenized\")\n",
    "print(len(corpus2))\n",
    "for i in corpus2.keys():\n",
    "    print(str(i)+\" is vectorizing!\")\n",
    "    vectorizer = TfidfVectorizer()\n",
    "#     vectorizer = TfidfVectorizer(\n",
    "#         min_df=0,\n",
    "#         max_df=1,\n",
    "#         ngram_range=(1,1),\n",
    "#         lowercase=True,\n",
    "#         tokenizer=lambda x:x.split())\n",
    "    pad_corpus = corpus2[i]\n",
    "    pad_corpus.extend(corpus2['기타'])\n",
    "    x = vectorizer.fit_transform(pad_corpus)\n",
    "    print(\"vectorizing is finished\")\n",
    "    p_path = './padded_'+str(category_dict[i])+'.pkl'\n",
    "#     p_path = './total.pkl'\n",
    "    with open(p_path, 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "#     mtx_path='./'+str(category_dict[i])+'.mtx'\n",
    "#     mmwrite(mtx_path,x)\n",
    "    print(p_path+\" is saved\")\n",
    "\n",
    "\n",
    "# # idx2vocab = 단어들 리스트 생성\n",
    "# idx2vocab = [vocab for vocab, idx in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])]\n",
    "# # ['term/tag'] \n",
    "# # print(idx2vocab[4300:4400])\n",
    "# print(idx2vocab[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472 is tokenizing\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "9999 is tokenized\n"
     ]
    }
   ],
   "source": [
    "# my_tokenizer = MyTokenizer(Twitter())\n",
    "# if tf matrix\n",
    "# vectorizer = CountVectorizer(tokenizer = my_tokenizer)\n",
    "#  if tf-idf matrix\n",
    "# vectorizer = TfidfVectorizer(tokenizer = my_tokenizer)\n",
    "# x = vectorizer.fit_transform(corpus['안전/환경'])\n",
    "\n",
    "my_tokenizer = MyTokenizer(Twitter())\n",
    "\n",
    "# # print(my_tokenizer(sent))\n",
    "\n",
    "print(str(k)+\" is tokenizing\")\n",
    "sub = list()\n",
    "for k,i in enumerate(corpus['기타'][:10000]):\n",
    "    tmp = my_tokenizer(i)\n",
    "    #     띄어쓰기로 분리된 \n",
    "    sub.append(' '.join(tmp))\n",
    "    if k%1000 == 0:\n",
    "        print(k)\n",
    "corpus2['기타'] = sub\n",
    "print(str(k)+\" is tokenized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 암호화폐 거래소 차단한다고 하는데 그럼 강원랜드 카지노 합법에 국민들에게 이로운지 알고 싶습니다 암호화폐 환경을 만들고 규제하시죠  강원랜드를 먼저 규제 하시죠  탁상행정을 하실려면 제대로 알고 접근하세요  is vectorizing!\n",
      "./total.pkl is saved\n",
      "252787\n"
     ]
    }
   ],
   "source": [
    "# corpus dict to (1,?) list \n",
    "# this one to make tfidf matrix and when test it'll be used\n",
    "corpus_list = list()\n",
    "for key,value in corpus2.items():\n",
    "    corpus_list.extend(value)\n",
    "print(str(i)+\" is vectorizing!\")\n",
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(corpus_list)\n",
    "p_path = './total.pkl'\n",
    "with open(p_path, 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "#     mtx_path='./'+str(category_dict[i])+'.mtx'\n",
    "#     mmwrite(mtx_path,x)\n",
    "print(p_path+\" is saved\")\n",
    "# #vectorizing 한 pickle 파일을 불러온다.\n",
    "# with open('./1.pkl', 'rb') as f:\n",
    "#     cate_1 = pickle.load(f)\n",
    "# print(cate_1)\n",
    "# print(len(corpus['미래']))\n",
    "print(len(corpus_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: test = 8:2\n",
    "t_rate = 0.8\n",
    "train_len = round(len(X_train)*t_rate)\n",
    "\n",
    "X_train = x[:train_len]\n",
    "X_test = x[train_len:]\n",
    "y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2['기타'] = corpus2['기타'][:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus2.json','w') as f:\n",
    "    json.dump(corpus2,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus2.json','r') as f:\n",
    "    json_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp= {'a':1,'b':78,'c':4}\n",
    "sorted_by_value = sorted(tmp.items(), key=lambda kv: kv[1],reverse=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': 78, 'c': 4, 'a': 1}\n"
     ]
    }
   ],
   "source": [
    "print(dict(sorted_by_value))\n",
    "# for k,v in sorted_by_value.items():\n",
    "#     print(k)\n",
    "#     print(v)\n",
    "    \n",
    "#     lk.extend(' '.join(i))\n",
    "    \n",
    "# with open(name,'w') as f:\n",
    "#     json.dump(sorted_by_value,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 1, 'c', 4, 'b', 78]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
