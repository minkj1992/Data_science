# 3. 언어모델(LM/ Grammer): 문장의 확률을 계산하거나, 또는 이전 단어들이 주어졌을 때, 다음 단어가 나올 확률을 계산하는 모델, 확률을 통해 단어들의 조합이 얼마나 적절한지, 또는 해당 문장이 얼마나 적합한지 알려주는 역활.

## 1. count 기반의 실제 언어 corpus모집단에 대한 샘플 데이터들의 훈련을 통하여 얻어진 approximation,조건부확률

ex)
An adorable little boy is
(즉 P(is| An adorable little boy)= count(An adorable little boy is)/count(An adorable little boy))

단점: 학습데이터가 어마무시하게 많이 필요하다. 학습 코퍼스에서 확률을 계산하고 싶은 문장이 없을 확률이 높다.

## 2. 마르코프의 가정과 그 이론을 이용한 n-gram

가정: P(is| An adorable little boy) ~ P(is|little boy)
예측 단어 앞에 제시된 문장 전체를 count하지 말고,앞 단어 중 임의의 n개만 포함해서 count를 해보자

n이 작으면, count상승 but 근사의 정확도는 점점 실제의 확률분포와 멀어진다. 반대로 n을 크게 잡으면 count 확률 떨어진다.


## 3. 딥러닝

-----------------------------------
c.f) 한글의 특성(영어와 비교해서)
1. 한글은 어순이 중요하지 않다. -> 이 때문에 확률에 기반한 언어 모델이 제대로 다음 단어를 예측하기 어렵다. (조건부 확률은 sequential하게 계산을 하니까) 

2. 다양한 조사가 나올 수 있다. 
그녀는, 그녀가 그녀를 그녀의 그녀와 그녀로-> 토큰화를 통해 접사나 조사를 분리해야 한다. 

3. 한글은 띄어쓰기 규칙이 까다롭다.
-> 띄어쓰기가 제대로 분리되지 않아 토큰이 제대로 분리 되지 않을 경우가 많다. 





















------------------------------------------------------------------------------
# 4.카운트 기반의 단어 표현

nlp에서 텍스트를 표현하는 방법
(1) Local representations
N-grams
원-핫 인코딩
Bag-of-words
(2) Continuous representations
Latent Semantic Analysis
Latent Dirichlet Allocation
Distributed Representations(딥러닝을 활용한 표현 방법)
---------------------------------------------------------------
## 1. 원핫인코딩
모든 단어에 고유한 숫자를 부여하는 것, 단어 set을 만들고 이를 index화 시킨다.이후 원핫벡터를 이용하여 단어를 표현한다.

한계: 
1) sparse하고 차원이 기하급수적으로 커진다.(차원의 저주) 
2) 단어끼리의 유사성을 표현하지 못한다. -> 단어의 의미를 다차원 공간에 벡터화 시키는 방법을 통해 해결(1. 카운트 기반의 단어 벡터화 LSA..., 2. 딥러닝 (NNLM,RNNLM,CBOW,skip-gram)

## 2. Bag of words(BoW)
BoW란 단어들의 순서는 전혀 고려하지 않고, 단어들의 frequency(출현빈도)에만 집중하는 텍스트 수치화 표현 법.

생성법
(1) 각 단어에 고유한 index를 부여word2index={} , 문장을 인덱스화 시킬 때 인덱스의 순서는 전혀 상관 없다.
(2) 각 인덱스의 위치에 토큰의 등장 횟수를 기록한 벡터를 생성 bow=[]  

특징: BoW는 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰입니다. 즉 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰입니다. 

Sklearn의 CountVectorizer를 사용하여 BoW만들 수 있다. 오직 띄어쓰기만을 기준으로 단어를 자른 후 BoW를 만드는 방식이므로 한글의 경우, tokenizer를 사용하여 토큰화 시킨후 작업을 진행해야 한다.

## 2'. 단어 문서 행렬(Term-Document Matrix, 서로 다른 문장들을 비교하기 위해 사용)
: 문장에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것.
: 각 문서에 대한 BoW를 하나의 행렬로 만든 것. (BoW의 표현 방법 중 하나, TDM이라 부른다.) 

The, a, an 과 같은 stopword들 제거가 필수적이다. 모든 문장에 공통적으로 들어가는 녀석들은 유사도 분석에서 -로 작용한다. 이런 stopword들과 중요한 단어에 대해서 가중치를 줄 수 있는 방법으로 TF-IDF를 사용한다.  

TF-IDF(단어 빈도-역문서 빈도, Term Frequency- Inverse Documnet Frequency)
: 주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰일 수 있다.


문서를 d, 단어를 t, 문서의 총 개수를 n

-tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수(TDM에서 각 문서 D에서의 각 단어t의 등장 빈도)
-df(t): 특정 단어 t가 등장한 문서의 수.(바나나가 26의 tf를 가져도 2개의 문서에 등장했다면 df = 2)
-idf(d,t):d개의 문서에  df에 반비례하는 수, 굳이 분모에 1을 더해주지 않고 epsilon을 더해주면 어떻게 될까?
-TF-IDF: TF와 IDF를 곱한 값
: TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단합니다. TF-IDF 값이 낮으면 중요도가 낮은 것이며, TF-IDF 값이 크면 중요도가 큰 것입니다. 즉, the나 a와 같이 불용어의 경우에는 모든 문서에 자주 등장하기 마련이기 때문에 자연스럽게 불용어의 TF-IDF의 값은 다른 단어의 TF-IDF에 비해서 낮아지게 됩니다.





######################
1. one hot repre
2. bag of words -> 등장횟수기반 term freq vector
tf/idf: 검색 엔진 분야에서 제안됨 문서와 query의 관계를 정의하기 위해서
ex['하정우','먹방'] 라는 query가 입력되었을 때 query와 상관있는 문서의 후보들을 만들려면 '하정우','먹방' 단어가 포함된 문서를 먼저 가져와야 한다. cos similarity를 사용하여 

3. Doc2Vec (distrubyter reoresentation/Word/Document embedding): 유사한 문서 찾기 

#### Konlpy
# 토크나이저/품사판별
.pos()에 입력하여 문장 형태소 확인가능
1. Hannanu
2. Kkma: 대명사(NNP)/일반명사(NNG)/ 이(지정사)+ㅂ니다(평서형 종결어미)
3. Mecab
4. Okt(Twitter): 명사/ 입니(형용사 어근)

.nouns() : 명사 추출




3. 클러스터링 
